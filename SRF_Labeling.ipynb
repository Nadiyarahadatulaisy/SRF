{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "AWXYwrPglW6r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/csvPH.csv' , sep=';', encoding='latin1')"
      ],
      "metadata": {
        "id": "KResKDTMlkP4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = [c for c in df.columns if c.lower() != 'index']"
      ],
      "metadata": {
        "id": "cxrGnMupltOB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bersihkan dan konversi koma ke titik, lalu numeric\n",
        "for c in num_cols:\n",
        "    df[c] = df[c].astype(str).str.replace(r'[^\\d,.\\-]', '', regex=True)\n",
        "    df[c] = df[c].str.replace(',', '.')\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "# Label sesuai aturan\n",
        "def label_row(row):\n",
        "    if (6 < row['pH'] < 7) and (20 < row['Temperatur (°C)'] < 30) and (20 < row['Kadar Air (%)'] < 40) and (60 < row['Kelembapan (%)'] < 80):\n",
        "        return 'ok'\n",
        "    else:\n",
        "        return 'warning'"
      ],
      "metadata": {
        "id": "vROBs2mxly_s"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df.apply(label_row, axis=1)\n",
        "\n",
        "# Fitur & target\n",
        "X = df[num_cols].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Encode label\n",
        "le = LabelEncoder(); y_enc = le.fit_transform(y)\n",
        "\n",
        "# Buang baris NaN bila ada\n",
        "mask = ~np.isnan(X).any(axis=1)\n",
        "X = X[mask]; y_enc = y_enc[mask]\n",
        "df = df.iloc[mask]\n",
        "\n",
        "# Simpan indeks asli\n",
        "indices = df.index.values\n",
        "\n",
        "# Split data beserta indeks\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, y_enc, indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_enc\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ANN (MLP)\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32,16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=15\n",
        ")\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Prediksi\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "y_pred_labels = le.inverse_transform(y_pred)\n",
        "\n",
        "# Tampilkan jumlah label setelah klasifikasi\n",
        "print(\"Jumlah label setelah klasifikasi:\")\n",
        "print(pd.Series(y_pred_labels).value_counts(), \"\\n\")\n",
        "\n",
        "# Indeks asli untuk prediksi 'ok' dan 'warning'\n",
        "ok_indices = idx_test[y_pred_labels == 'ok']\n",
        "warning_indices = idx_test[y_pred_labels == 'warning']\n",
        "\n",
        "print(\"Indeks asli prediksi 'ok'     :\", ok_indices.tolist())\n",
        "print(\"Indeks asli prediksi 'warning':\", warning_indices.tolist(), \"\\n\")\n",
        "\n",
        "# Evaluasi model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r_F0_YYl6TT",
        "outputId": "c2167591-954d-41cc-e766-5ed69ce303b2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah label setelah klasifikasi:\n",
            "warning    1000\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Indeks asli prediksi 'ok'     : []\n",
            "Indeks asli prediksi 'warning': [360, 1153, 1626, 2070, 4680, 304, 3282, 4517, 422, 1285, 1360, 3920, 67, 2706, 2708, 4537, 2214, 818, 2088, 4837, 4650, 252, 1917, 2343, 727, 3732, 3294, 819, 2523, 514, 2312, 1693, 4674, 3073, 4929, 1026, 1671, 2479, 4430, 1984, 2224, 774, 667, 3728, 3493, 4363, 4574, 4386, 3015, 2405, 3162, 1339, 1260, 992, 2074, 4160, 3067, 961, 3288, 2409, 1086, 4442, 3250, 2461, 4963, 3910, 4396, 649, 2992, 1978, 1253, 3696, 1313, 1915, 1171, 4230, 855, 288, 4054, 2296, 2301, 4207, 4035, 2853, 1824, 2788, 3507, 3009, 675, 4972, 4868, 3602, 2973, 2234, 78, 4012, 3103, 128, 1836, 4521, 4762, 4583, 1141, 3797, 2781, 3267, 1612, 620, 3088, 2934, 4648, 1196, 3618, 959, 1219, 2606, 1979, 976, 148, 4661, 2050, 1307, 903, 432, 4588, 2406, 2371, 766, 1146, 2948, 3136, 9, 2397, 2035, 3252, 2265, 898, 4304, 1723, 3655, 2658, 4873, 2576, 4803, 3664, 3299, 1953, 4368, 4168, 2497, 225, 2253, 1534, 3120, 1257, 156, 923, 2573, 2044, 4119, 1338, 3317, 4513, 226, 1222, 287, 1160, 2748, 2703, 3364, 407, 850, 3755, 4997, 1732, 1800, 2466, 1592, 928, 2058, 4571, 772, 4530, 1500, 3238, 4913, 831, 3032, 899, 567, 3663, 3199, 1467, 2131, 2823, 3871, 1741, 2792, 2114, 3228, 1873, 3519, 1990, 3749, 4358, 2950, 2338, 420, 2625, 4869, 940, 3436, 4763, 2491, 191, 4008, 4866, 1675, 3462, 3489, 4307, 4557, 1303, 3990, 1864, 2760, 4066, 4379, 1652, 1581, 4498, 129, 2608, 2398, 1939, 3046, 1535, 4749, 4345, 1769, 3897, 1210, 905, 4758, 2911, 4192, 2926, 4063, 3106, 2991, 2980, 4902, 4435, 1123, 4028, 1536, 1007, 2160, 1491, 3466, 1082, 3945, 1862, 2439, 3438, 4208, 4743, 149, 382, 2831, 2162, 3277, 238, 3523, 3437, 4112, 808, 2847, 1638, 2125, 2946, 1391, 2976, 980, 2213, 3620, 1370, 4836, 4511, 4156, 2962, 2354, 3754, 2047, 1580, 3563, 4137, 492, 4761, 650, 3974, 1689, 852, 3578, 4298, 1700, 4474, 398, 2069, 2563, 3738, 3547, 1485, 2886, 2288, 2168, 3764, 4455, 1842, 1810, 4461, 3297, 2170, 1017, 4886, 958, 1112, 3597, 4654, 1802, 1861, 3763, 3529, 3478, 1144, 4014, 3742, 1064, 955, 4382, 145, 4657, 4407, 1174, 1835, 3964, 1999, 874, 4951, 1871, 2974, 1849, 4091, 4770, 527, 1087, 1376, 779, 4792, 283, 1865, 99, 3758, 4987, 4796, 566, 2347, 3599, 3832, 1967, 459, 3386, 3768, 3126, 3359, 989, 394, 1761, 1694, 2756, 2648, 3896, 1507, 478, 1127, 3766, 1632, 4977, 4271, 2210, 3453, 4221, 4667, 4955, 3614, 2090, 1020, 2551, 1068, 880, 2203, 118, 1831, 1715, 2638, 1767, 4920, 726, 3867, 3310, 4572, 2379, 4351, 1549, 1538, 4578, 639, 4614, 2137, 2304, 540, 3076, 939, 2571, 1670, 926, 162, 2700, 829, 732, 2771, 2745, 3124, 3590, 770, 2006, 922, 3870, 950, 329, 2570, 1523, 1477, 857, 1860, 2561, 3133, 1395, 3474, 2247, 2725, 2215, 2581, 3715, 4684, 932, 305, 265, 1578, 323, 2610, 3414, 2198, 2839, 3838, 1943, 4693, 1502, 2064, 496, 4320, 2520, 63, 263, 2456, 797, 4042, 3843, 3515, 4200, 4811, 2275, 4964, 1125, 65, 4224, 2982, 528, 245, 3373, 4482, 3021, 3193, 4162, 2864, 2455, 570, 3303, 4518, 1655, 2988, 4180, 344, 3309, 391, 235, 1832, 1887, 2229, 1703, 1325, 1993, 3691, 4178, 4938, 2885, 1050, 2798, 474, 4686, 2943, 996, 3720, 584, 607, 604, 3320, 195, 135, 4494, 3191, 1189, 2846, 4632, 3411, 3434, 2516, 3894, 627, 4290, 4319, 3933, 1248, 503, 3218, 2929, 119, 3163, 670, 4966, 3370, 4380, 2036, 4607, 1977, 1081, 3102, 4834, 3025, 1165, 1155, 2694, 2503, 3756, 3860, 3524, 1255, 1991, 3795, 3512, 202, 3463, 612, 2009, 2906, 1709, 2971, 3138, 1499, 2868, 2536, 3283, 35, 971, 3351, 4039, 95, 3534, 2003, 4100, 3343, 4395, 4852, 4477, 3157, 4824, 163, 3390, 4, 2829, 721, 3654, 2200, 1416, 2801, 3595, 1223, 3983, 858, 1968, 3406, 1818, 2447, 3503, 4213, 1685, 1161, 3134, 415, 4283, 529, 679, 1321, 2161, 2893, 4908, 2673, 2634, 2005, 2514, 3694, 2056, 1403, 55, 2388, 2621, 4445, 2484, 3083, 348, 203, 2626, 599, 3244, 3943, 4606, 199, 116, 1964, 3786, 2721, 2998, 3391, 1742, 1610, 2549, 3550, 2749, 2150, 3863, 3600, 2494, 4186, 4718, 4754, 4601, 4535, 4514, 3152, 2639, 2310, 1152, 4844, 2272, 1814, 2152, 1730, 2953, 3776, 3085, 3539, 2884, 985, 4165, 506, 1846, 16, 2145, 2716, 244, 2650, 1150, 4082, 3527, 4366, 2335, 1686, 879, 2175, 1281, 1765, 4581, 4240, 4823, 3330, 365, 2913, 3393, 139, 1021, 2726, 1856, 3072, 4944, 3491, 994, 942, 1298, 1643, 1355, 3047, 757, 4722, 3457, 1159, 387, 1273, 3173, 2374, 2501, 3207, 204, 1653, 2202, 2738, 5, 1611, 1365, 2467, 4839, 2722, 4914, 2916, 834, 481, 4531, 1640, 4626, 1851, 4154, 4670, 563, 2743, 4193, 2927, 3, 2747, 1827, 4228, 2084, 3645, 3224, 3461, 1680, 1926, 954, 192, 1091, 663, 2876, 121, 4402, 3262, 801, 4002, 2812, 4373, 1908, 2048, 1207, 3341, 1401, 1372, 876, 1896, 671, 208, 642, 3573, 2578, 3075, 4153, 3615, 2794, 3449, 2306, 3084, 2688, 4949, 3789, 4128, 3089, 3346, 2821, 2337, 1575, 4322, 3361, 4970, 227, 3221, 2026, 2190, 720, 3115, 703, 1319, 1646, 4759, 2430, 3973, 4822, 3782, 4622, 3783, 4114, 665, 3818, 2603, 3300, 1336, 3358, 1547, 610, 2057, 1289, 4747, 2919, 964, 4988, 4120, 2360, 4392, 692, 471, 2687, 1262, 3874, 4795, 1071, 508, 2887, 1904, 744, 717, 3963, 2903, 4088, 702, 2938, 4550, 3168, 1331, 4129, 1940, 3235, 2569, 258, 4385, 4197, 2065, 573, 4499, 4078, 2535, 2271, 2007, 1282, 543, 827, 2744, 388, 1929, 4352, 3811, 4899, 3706, 3817, 1065, 4173, 2762, 3186, 2752, 1033, 2382, 2826, 2079, 4510, 2965, 367, 1452, 4496, 2071, 713, 4487, 764, 2134, 2854, 3995, 1172, 3482, 3883, 4334, 2753, 1000, 25, 867, 338, 736, 4741, 677, 2091, 3662, 4967, 4505, 2171, 788, 3231, 882, 3070, 4805, 2797, 1208, 1985, 3340, 1727, 3258, 3156, 4515, 541, 3385, 4164, 3271, 3035, 3465, 4683, 4210, 4422, 4364, 3904, 4048, 2558, 2166, 516, 1588, 3408, 1074, 4585, 4158, 2731, 4896, 56, 3734, 1704, 1604, 378, 4145, 4383, 4021, 1070, 4880, 2759, 2805, 2629, 127, 1067, 1642, 2373, 4708, 3140, 1390, 3384, 1995, 4050, 4411, 3548, 4800, 3053, 3318, 2789, 1687, 3531, 578, 340, 2714, 1577, 1349, 997, 256, 2777] \n",
            "\n",
            "Accuracy: 0.995\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ok       0.00      0.00      0.00         5\n",
            "     warning       0.99      1.00      1.00       995\n",
            "\n",
            "    accuracy                           0.99      1000\n",
            "   macro avg       0.50      0.50      0.50      1000\n",
            "weighted avg       0.99      0.99      0.99      1000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  0   5]\n",
            " [  0 995]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/csvPH.csv', sep=';', encoding='latin1')\n",
        "\n",
        "# Pilih kolom numerik (kecuali Index)\n",
        "num_cols = [c for c in df.columns if c.lower() != 'index']\n",
        "\n",
        "# Bersihkan data numerik\n",
        "for c in num_cols:\n",
        "    df[c] = df[c].astype(str).str.replace(r'[^\\d,.\\-]', '', regex=True)\n",
        "    df[c] = df[c].str.replace(',', '.')\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "# Buat label\n",
        "def label_row(row):\n",
        "    if (6 < row['pH'] < 7) and (20 < row['Temperatur (°C)'] < 30) and (20 < row['Kadar Air (%)'] < 40) and (60 < row['Kelembapan (%)'] < 80):\n",
        "        return 'ok'\n",
        "    else:\n",
        "        return 'warning'\n",
        "\n",
        "df['label'] = df.apply(label_row, axis=1)\n",
        "\n",
        "# Distribusi label sebelum klasifikasi\n",
        "print(\"Distribusi label awal:\")\n",
        "print(df['label'].value_counts(), \"\\n\")\n",
        "\n",
        "# Fitur & target\n",
        "X = df[num_cols].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Encode label\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "# Buang baris NaN\n",
        "mask = ~np.isnan(X).any(axis=1)\n",
        "X = X[mask]\n",
        "y_enc = y_enc[mask]\n",
        "df = df.iloc[mask]\n",
        "\n",
        "# Simpan indeks asli\n",
        "indices = df.index.values\n",
        "\n",
        "# Split data + indeks\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, y_enc, indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_enc\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- SMOTE Oversampling ---\n",
        "print(\"Distribusi sebelum SMOTE:\", np.bincount(y_train))\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
        "print(\"Distribusi sesudah SMOTE:\", np.bincount(y_train_res), \"\\n\")\n",
        "\n",
        "# ANN\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(32,16),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=15\n",
        ")\n",
        "mlp.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Prediksi\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "y_pred_labels = le.inverse_transform(y_pred)\n",
        "\n",
        "# Jumlah label setelah klasifikasi\n",
        "print(\"Distribusi label setelah klasifikasi:\")\n",
        "print(pd.Series(y_pred_labels).value_counts(), \"\\n\")\n",
        "\n",
        "# Indeks asli untuk prediksi 'ok' dan 'warning'\n",
        "ok_indices = idx_test[y_pred_labels == 'ok']\n",
        "warning_indices = idx_test[y_pred_labels == 'warning']\n",
        "\n",
        "print(\"Indeks asli prediksi 'ok'     :\", ok_indices.tolist())\n",
        "print(\"Indeks asli prediksi 'warning':\", warning_indices.tolist(), \"\\n\")\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idNaCXz2y3OC",
        "outputId": "43c196c7-b0cd-41f8-e9a8-a4626f678565"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi label awal:\n",
            "label\n",
            "warning    4973\n",
            "ok           27\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Distribusi sebelum SMOTE: [  22 3978]\n",
            "Distribusi sesudah SMOTE: [3978 3978] \n",
            "\n",
            "Distribusi label setelah klasifikasi:\n",
            "warning    990\n",
            "ok          10\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Indeks asli prediksi 'ok'     : [1285, 2312, 1694, 2756, 1703, 4499, 2071, 4334, 1704, 2373]\n",
            "Indeks asli prediksi 'warning': [360, 1153, 1626, 2070, 4680, 304, 3282, 4517, 422, 1360, 3920, 67, 2706, 2708, 4537, 2214, 818, 2088, 4837, 4650, 252, 1917, 2343, 727, 3732, 3294, 819, 2523, 514, 1693, 4674, 3073, 4929, 1026, 1671, 2479, 4430, 1984, 2224, 774, 667, 3728, 3493, 4363, 4574, 4386, 3015, 2405, 3162, 1339, 1260, 992, 2074, 4160, 3067, 961, 3288, 2409, 1086, 4442, 3250, 2461, 4963, 3910, 4396, 649, 2992, 1978, 1253, 3696, 1313, 1915, 1171, 4230, 855, 288, 4054, 2296, 2301, 4207, 4035, 2853, 1824, 2788, 3507, 3009, 675, 4972, 4868, 3602, 2973, 2234, 78, 4012, 3103, 128, 1836, 4521, 4762, 4583, 1141, 3797, 2781, 3267, 1612, 620, 3088, 2934, 4648, 1196, 3618, 959, 1219, 2606, 1979, 976, 148, 4661, 2050, 1307, 903, 432, 4588, 2406, 2371, 766, 1146, 2948, 3136, 9, 2397, 2035, 3252, 2265, 898, 4304, 1723, 3655, 2658, 4873, 2576, 4803, 3664, 3299, 1953, 4368, 4168, 2497, 225, 2253, 1534, 3120, 1257, 156, 923, 2573, 2044, 4119, 1338, 3317, 4513, 226, 1222, 287, 1160, 2748, 2703, 3364, 407, 850, 3755, 4997, 1732, 1800, 2466, 1592, 928, 2058, 4571, 772, 4530, 1500, 3238, 4913, 831, 3032, 899, 567, 3663, 3199, 1467, 2131, 2823, 3871, 1741, 2792, 2114, 3228, 1873, 3519, 1990, 3749, 4358, 2950, 2338, 420, 2625, 4869, 940, 3436, 4763, 2491, 191, 4008, 4866, 1675, 3462, 3489, 4307, 4557, 1303, 3990, 1864, 2760, 4066, 4379, 1652, 1581, 4498, 129, 2608, 2398, 1939, 3046, 1535, 4749, 4345, 1769, 3897, 1210, 905, 4758, 2911, 4192, 2926, 4063, 3106, 2991, 2980, 4902, 4435, 1123, 4028, 1536, 1007, 2160, 1491, 3466, 1082, 3945, 1862, 2439, 3438, 4208, 4743, 149, 382, 2831, 2162, 3277, 238, 3523, 3437, 4112, 808, 2847, 1638, 2125, 2946, 1391, 2976, 980, 2213, 3620, 1370, 4836, 4511, 4156, 2962, 2354, 3754, 2047, 1580, 3563, 4137, 492, 4761, 650, 3974, 1689, 852, 3578, 4298, 1700, 4474, 398, 2069, 2563, 3738, 3547, 1485, 2886, 2288, 2168, 3764, 4455, 1842, 1810, 4461, 3297, 2170, 1017, 4886, 958, 1112, 3597, 4654, 1802, 1861, 3763, 3529, 3478, 1144, 4014, 3742, 1064, 955, 4382, 145, 4657, 4407, 1174, 1835, 3964, 1999, 874, 4951, 1871, 2974, 1849, 4091, 4770, 527, 1087, 1376, 779, 4792, 283, 1865, 99, 3758, 4987, 4796, 566, 2347, 3599, 3832, 1967, 459, 3386, 3768, 3126, 3359, 989, 394, 1761, 2648, 3896, 1507, 478, 1127, 3766, 1632, 4977, 4271, 2210, 3453, 4221, 4667, 4955, 3614, 2090, 1020, 2551, 1068, 880, 2203, 118, 1831, 1715, 2638, 1767, 4920, 726, 3867, 3310, 4572, 2379, 4351, 1549, 1538, 4578, 639, 4614, 2137, 2304, 540, 3076, 939, 2571, 1670, 926, 162, 2700, 829, 732, 2771, 2745, 3124, 3590, 770, 2006, 922, 3870, 950, 329, 2570, 1523, 1477, 857, 1860, 2561, 3133, 1395, 3474, 2247, 2725, 2215, 2581, 3715, 4684, 932, 305, 265, 1578, 323, 2610, 3414, 2198, 2839, 3838, 1943, 4693, 1502, 2064, 496, 4320, 2520, 63, 263, 2456, 797, 4042, 3843, 3515, 4200, 4811, 2275, 4964, 1125, 65, 4224, 2982, 528, 245, 3373, 4482, 3021, 3193, 4162, 2864, 2455, 570, 3303, 4518, 1655, 2988, 4180, 344, 3309, 391, 235, 1832, 1887, 2229, 1325, 1993, 3691, 4178, 4938, 2885, 1050, 2798, 474, 4686, 2943, 996, 3720, 584, 607, 604, 3320, 195, 135, 4494, 3191, 1189, 2846, 4632, 3411, 3434, 2516, 3894, 627, 4290, 4319, 3933, 1248, 503, 3218, 2929, 119, 3163, 670, 4966, 3370, 4380, 2036, 4607, 1977, 1081, 3102, 4834, 3025, 1165, 1155, 2694, 2503, 3756, 3860, 3524, 1255, 1991, 3795, 3512, 202, 3463, 612, 2009, 2906, 1709, 2971, 3138, 1499, 2868, 2536, 3283, 35, 971, 3351, 4039, 95, 3534, 2003, 4100, 3343, 4395, 4852, 4477, 3157, 4824, 163, 3390, 4, 2829, 721, 3654, 2200, 1416, 2801, 3595, 1223, 3983, 858, 1968, 3406, 1818, 2447, 3503, 4213, 1685, 1161, 3134, 415, 4283, 529, 679, 1321, 2161, 2893, 4908, 2673, 2634, 2005, 2514, 3694, 2056, 1403, 55, 2388, 2621, 4445, 2484, 3083, 348, 203, 2626, 599, 3244, 3943, 4606, 199, 116, 1964, 3786, 2721, 2998, 3391, 1742, 1610, 2549, 3550, 2749, 2150, 3863, 3600, 2494, 4186, 4718, 4754, 4601, 4535, 4514, 3152, 2639, 2310, 1152, 4844, 2272, 1814, 2152, 1730, 2953, 3776, 3085, 3539, 2884, 985, 4165, 506, 1846, 16, 2145, 2716, 244, 2650, 1150, 4082, 3527, 4366, 2335, 1686, 879, 2175, 1281, 1765, 4581, 4240, 4823, 3330, 365, 2913, 3393, 139, 1021, 2726, 1856, 3072, 4944, 3491, 994, 942, 1298, 1643, 1355, 3047, 757, 4722, 3457, 1159, 387, 1273, 3173, 2374, 2501, 3207, 204, 1653, 2202, 2738, 5, 1611, 1365, 2467, 4839, 2722, 4914, 2916, 834, 481, 4531, 1640, 4626, 1851, 4154, 4670, 563, 2743, 4193, 2927, 3, 2747, 1827, 4228, 2084, 3645, 3224, 3461, 1680, 1926, 954, 192, 1091, 663, 2876, 121, 4402, 3262, 801, 4002, 2812, 4373, 1908, 2048, 1207, 3341, 1401, 1372, 876, 1896, 671, 208, 642, 3573, 2578, 3075, 4153, 3615, 2794, 3449, 2306, 3084, 2688, 4949, 3789, 4128, 3089, 3346, 2821, 2337, 1575, 4322, 3361, 4970, 227, 3221, 2026, 2190, 720, 3115, 703, 1319, 1646, 4759, 2430, 3973, 4822, 3782, 4622, 3783, 4114, 665, 3818, 2603, 3300, 1336, 3358, 1547, 610, 2057, 1289, 4747, 2919, 964, 4988, 4120, 2360, 4392, 692, 471, 2687, 1262, 3874, 4795, 1071, 508, 2887, 1904, 744, 717, 3963, 2903, 4088, 702, 2938, 4550, 3168, 1331, 4129, 1940, 3235, 2569, 258, 4385, 4197, 2065, 573, 4078, 2535, 2271, 2007, 1282, 543, 827, 2744, 388, 1929, 4352, 3811, 4899, 3706, 3817, 1065, 4173, 2762, 3186, 2752, 1033, 2382, 2826, 2079, 4510, 2965, 367, 1452, 4496, 713, 4487, 764, 2134, 2854, 3995, 1172, 3482, 3883, 2753, 1000, 25, 867, 338, 736, 4741, 677, 2091, 3662, 4967, 4505, 2171, 788, 3231, 882, 3070, 4805, 2797, 1208, 1985, 3340, 1727, 3258, 3156, 4515, 541, 3385, 4164, 3271, 3035, 3465, 4683, 4210, 4422, 4364, 3904, 4048, 2558, 2166, 516, 1588, 3408, 1074, 4585, 4158, 2731, 4896, 56, 3734, 1604, 378, 4145, 4383, 4021, 1070, 4880, 2759, 2805, 2629, 127, 1067, 1642, 4708, 3140, 1390, 3384, 1995, 4050, 4411, 3548, 4800, 3053, 3318, 2789, 1687, 3531, 578, 340, 2714, 1577, 1349, 997, 256, 2777] \n",
            "\n",
            "Accuracy: 0.993\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ok       0.40      0.80      0.53         5\n",
            "     warning       1.00      0.99      1.00       995\n",
            "\n",
            "    accuracy                           0.99      1000\n",
            "   macro avg       0.70      0.90      0.76      1000\n",
            "weighted avg       1.00      0.99      0.99      1000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  4   1]\n",
            " [  6 989]]\n"
          ]
        }
      ]
    }
  ]
}